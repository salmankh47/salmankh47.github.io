<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.2.2">Jekyll</generator><link href="https://salmankh47.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://salmankh47.github.io/" rel="alternate" type="text/html" hreflang="en" /><updated>2022-05-29T17:42:57+05:30</updated><id>https://salmankh47.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design.
</subtitle><entry><title type="html">JAX. A quick peek</title><link href="https://salmankh47.github.io/blog/2022/jax-1/" rel="alternate" type="text/html" title="JAX. A quick peek" /><published>2022-05-02T00:00:00+05:30</published><updated>2022-05-02T00:00:00+05:30</updated><id>https://salmankh47.github.io/blog/2022/jax-1</id><content type="html" xml:base="https://salmankh47.github.io/blog/2022/jax-1/"><![CDATA[<h1 id="introduction-to-jax">Introduction to JAX</h1>

<p>JAX is a GPU/TPU accelerated API developed by Google, that allows transformations and manipulation of Numpy-like arrays. Essentially, JAX is a Numpy on steroids.</p>

<p>Jax make use of XLA (Accelerated Linear Algebra) to compile and run code on hardware accelerators like GPU/TPU. Where Autograd allows for automatic differentiation (both forward and reverse) of python function to arbitrary order. JAX system make use of just-in-time compiler to generate code for pure-and-statically-composed subroutines. It can perform composible transformations of numpy array to differentiate, vectorize and parallalize, just-in-time compilation and much much more. 
Let us begin.</p>

<h2 id="installation">Installation</h2>
<p>JAX for CPU can be installed by running the following command on a shell. JAX depends on XLA, which needs to be installed as <code class="language-plaintext highlighter-rouge">jaxlib</code> package. Use the following instruction.</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>   <span class="nv">$ </span>pip <span class="nb">install</span> <span class="nt">--upgrade</span> pip
   <span class="nv">$ </span>pip <span class="nb">install</span> <span class="nt">--upgrade</span> <span class="s2">"jax[cpu]"</span>
</code></pre></div></div>
<p>JAX with NVidia GPU support, require CUDA and CUDNN. JAX is not bundled with CUDA and CUDNN with pip package. JAX provides pre-built CUDA-compatible wheels for Linux only, with CUDA 11.1 or newer, and CuDNN 8.0.5 or newer. Other combinations of operating system, CUDA, and CuDNN are possible, but require building from source. A detailed instruction can be found <a href="https://github.com/google/jax#installation">here</a></p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="nv">$ </span>pip <span class="nb">install</span> <span class="nt">--upgrade</span> pip
  <span class="nv">$ </span>pip <span class="nb">install</span> <span class="nt">--upgrade</span> <span class="s2">"jax[cuda]"</span> <span class="nt">-f</span> https://storage.googleapis.com/jax-releases/jax_releases.html  
</code></pre></div></div>
<h3 id="check-if-jax-is-using-gpu">Check if <code class="language-plaintext highlighter-rouge">jax</code> is using GPU</h3>
<p>In order to check whether <code class="language-plaintext highlighter-rouge">jaxlib</code> identifies your hardware accelerator. It can be done in any of the following method</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
</pre></td><td class="code"><pre>  <span class="kn">import</span> <span class="nn">jax</span>
  <span class="n">jax</span><span class="p">.</span><span class="n">default_backend</span><span class="p">()</span>
</pre></td></tr></tbody></table></code></pre></figure>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>gpu
</code></pre></div></div>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
</pre></td><td class="code"><pre>  <span class="n">jax</span><span class="p">.</span><span class="n">devices</span><span class="p">()</span>
</pre></td></tr></tbody></table></code></pre></figure>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[GpuDevice(id=0, process_index=0), GpuDevice(id=1, process_index=0)]
</code></pre></div></div>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
</pre></td><td class="code"><pre>  <span class="n">jax</span><span class="p">.</span><span class="n">device_count</span><span class="p">()</span> <span class="c1"># shows number of devices</span>
</pre></td></tr></tbody></table></code></pre></figure>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>2
</code></pre></div></div>

<h2 id="jax-numpy-vs-numpy">Jax-numpy vs Numpy</h2>
<p>JAX-numpy and regular numpy are very very similar in terms of API. Most operations in numpy is that is available in numpy is implemented in JAX-numpy also. We can just take a look at a few.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
</pre></td><td class="code"><pre>  <span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
  <span class="kn">import</span> <span class="nn">jax.numpy</span> <span class="k">as</span> <span class="n">jnp</span>

  <span class="n">array_np</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">int32</span><span class="p">)</span>
  <span class="n">array_jnp</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">jnp</span><span class="p">.</span><span class="n">int32</span><span class="p">)</span>

  <span class="k">print</span><span class="p">(</span><span class="s">"Numpy array: "</span><span class="p">,</span> <span class="n">array_np</span><span class="p">)</span>
  <span class="k">print</span><span class="p">(</span><span class="s">"JAX array: "</span><span class="p">,</span> <span class="n">array_jnp</span><span class="p">)</span>

  <span class="k">print</span><span class="p">(</span><span class="s">"numpy array type:"</span><span class="p">,</span> <span class="nb">type</span><span class="p">(</span><span class="n">array_np</span><span class="p">))</span>
  <span class="k">print</span><span class="p">(</span><span class="s">"jax-numpy array type: "</span><span class="p">,</span> <span class="nb">type</span><span class="p">(</span><span class="n">array_jnp</span><span class="p">))</span>
</pre></td></tr></tbody></table></code></pre></figure>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Numpy array:  [0 1 2 3 4 5 6 7 8 9]
JAX array:  [0 1 2 3 4 5 6 7 8 9]
type numpy:  &lt;class 'numpy.ndarray'&gt;
type jax-numpy:  &lt;class 'jaxlib.xla_extension.DeviceArray'&gt;

</code></pre></div></div>
<p>Lets plot some graphs.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
</pre></td><td class="code"><pre>  <span class="kn">import</span> <span class="nn">jax</span>
  <span class="kn">import</span> <span class="nn">jax.numpy</span> <span class="k">as</span> <span class="n">jnp</span>
  <span class="kn">from</span> <span class="nn">jax</span> <span class="kn">import</span> <span class="n">jit</span> 
  <span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
  <span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>
</pre></td></tr></tbody></table></code></pre></figure>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
</pre></td><td class="code"><pre>  <span class="n">x_np</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span>
  <span class="n">y_np</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">sin</span><span class="p">(</span><span class="n">x_np</span><span class="p">)</span> <span class="o">+</span> <span class="n">x_np</span> <span class="o">**</span> <span class="mf">0.5</span>
  <span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_np</span><span class="p">,</span> <span class="n">y_np</span><span class="p">)</span>
  <span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</pre></td></tr></tbody></table></code></pre></figure>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
	<figure>

  <picture>
    <source media="(max-width: 480px)" srcset="/assets/img/jax/output_1_0-480.webp" />
    <source media="(max-width: 800px)" srcset="/assets/img/jax/output_1_0-800.webp" />
    <source media="(max-width: 1400px)" srcset="/assets/img/jax/output_1_0-1400.webp" />
    <!-- Fallback to the original file -->
    <img class="img-fluid rounded z-depth-0" src="/assets/img/jax/output_1_0.png" />

  </picture>

</figure>

    </div>
</div>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
</pre></td><td class="code"><pre>  <span class="n">x_jnp</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span>
  <span class="n">y_jnp</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="n">sin</span><span class="p">(</span><span class="n">x_jnp</span><span class="p">)</span> <span class="o">+</span> <span class="n">x_jnp</span> <span class="o">**</span> <span class="mf">0.5</span>
  <span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_jnp</span><span class="p">,</span> <span class="n">y_jnp</span><span class="p">)</span>
  <span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</pre></td></tr></tbody></table></code></pre></figure>

<div class="row">
  <figure>

  <picture>
    <source media="(max-width: 480px)" srcset="/assets/img/jax/output_1_0-480.webp" />
    <source media="(max-width: 800px)" srcset="/assets/img/jax/output_1_0-800.webp" />
    <source media="(max-width: 1400px)" srcset="/assets/img/jax/output_1_0-1400.webp" />
    <!-- Fallback to the original file -->
    <img class="rounded z-depth-0" src="/assets/img/jax/output_1_0.png" />

  </picture>

</figure>

</div>

<p>This is just a sample on the parallel between JAX and numpy. The real power of JAX comes with its ability to use hardware accelerators for operations. Lets look at the following example.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
</pre></td><td class="code"><pre>  <span class="n">seed</span> <span class="o">=</span> <span class="mi">0</span>
  <span class="n">key</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">PRNGKey</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
  <span class="n">size</span> <span class="o">=</span> <span class="mi">5000</span>

  <span class="n">x_jnp</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">normal</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="n">size</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">jnp</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>
  <span class="n">x_np</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">normal</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="n">size</span><span class="p">)).</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>


  <span class="o">%</span><span class="n">timeit</span> <span class="n">jnp</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x_jnp</span><span class="p">,</span> <span class="n">x_jnp</span><span class="p">.</span><span class="n">T</span><span class="p">).</span><span class="n">block_until_ready</span><span class="p">()</span>
  <span class="o">%</span><span class="n">timeit</span> <span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x_np</span><span class="p">,</span> <span class="n">x_np</span><span class="p">.</span><span class="n">T</span><span class="p">)</span>
  <span class="o">%</span><span class="n">timeit</span> <span class="n">jnp</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x_np</span><span class="p">,</span> <span class="n">x_np</span><span class="p">.</span><span class="n">T</span><span class="p">).</span><span class="n">block_until_ready</span><span class="p">()</span>
</pre></td></tr></tbody></table></code></pre></figure>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>27.8 ms ± 131 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)
355 ms ± 780 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)
120 ms ± 14.6 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
</code></pre></div></div>

<p>We can see the drastic performance change in the first and second line of code. Where the first one uses JAX device array and JAX implimentation of dot product which uses GPU for computation and second one uses regular numpy array with no hardware acceleration. The third line of code uses jax operations on numpy arrays. We will visit a detailed analysis on this in the future. For now lets focus on the performance gain using <code class="language-plaintext highlighter-rouge">jax</code></p>

<h3 id="accelerate-using-jit-function">Accelerate using <code class="language-plaintext highlighter-rouge">jit()</code> function</h3>
<p><code class="language-plaintext highlighter-rouge">jit()</code> is special function for accelerating jax executions using just-in-time compilation and vectorization respectively. We will cover deeply on these topics on upcoming posts. For now lets see what kind of performance improvements we are expecting.</p>

<h4 id="just-in-time-jit-compilation">Just-in-time, <code class="language-plaintext highlighter-rouge">jit()</code> compilation</h4>
<p>Let us start by defining some helper functions and some jax-numpy vectors</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
</pre></td><td class="code"><pre>  <span class="n">size</span> <span class="o">=</span> <span class="mi">1000</span>

  <span class="n">x_jnp</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">normal</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="n">size</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">jnp</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>


  <span class="k">def</span> <span class="nf">selu</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">1.67</span><span class="p">,</span> <span class="n">lmbda</span><span class="o">=</span><span class="mf">1.05</span><span class="p">):</span>  <span class="c1"># note: SELU is an activation function
</span>    <span class="k">return</span> <span class="n">lmbda</span> <span class="o">*</span> <span class="n">jnp</span><span class="p">.</span><span class="n">where</span><span class="p">(</span><span class="n">x</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">jnp</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">-</span> <span class="n">alpha</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">jnp</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">))</span>


  <span class="k">def</span> <span class="nf">add</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">jnp</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="o">*</span> <span class="n">jnp</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">-</span> <span class="mf">2.05</span> <span class="o">+</span> <span class="n">y</span> <span class="o">*</span> <span class="mf">0.56</span>

  <span class="c1"># Simple helper visualization function
</span>
  <span class="k">def</span> <span class="nf">visualize_fn</span><span class="p">(</span><span class="n">fn</span><span class="p">,</span> <span class="n">l</span><span class="o">=-</span><span class="mi">10</span><span class="p">,</span> <span class="n">r</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="mi">1000</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">l</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">num</span><span class="o">=</span><span class="n">n</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">fn</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">);</span> <span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>


  <span class="n">sigmoid_jit</span> <span class="o">=</span> <span class="n">jit</span><span class="p">(</span><span class="n">sigmoid</span><span class="p">)</span>
  <span class="n">selu_jit</span> <span class="o">=</span> <span class="n">jit</span><span class="p">(</span><span class="n">selu</span><span class="p">)</span>
  <span class="n">add_jit</span> <span class="o">=</span> <span class="n">jit</span><span class="p">(</span><span class="n">add</span><span class="p">)</span>
  <span class="n">visualize_fn</span><span class="p">(</span><span class="n">selu</span><span class="p">)</span>
  <span class="n">visualize_fn</span><span class="p">(</span><span class="n">sigmoid</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></figure>

<p>Once the functions are defined we use <code class="language-plaintext highlighter-rouge">@jit</code> decorator for speedup. Normally JAX dispatches tasks to GPU one operation at a time. But if we have a sequence of operation, we can compile multiple operations together using XLA and despatch to GPU.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
	<figure>

  <picture>
    <source media="(max-width: 480px)" srcset="/assets/img/jax/output_17_0-480.webp" />
    <source media="(max-width: 800px)" srcset="/assets/img/jax/output_17_0-800.webp" />
    <source media="(max-width: 1400px)" srcset="/assets/img/jax/output_17_0-1400.webp" />
    <!-- Fallback to the original file -->
    <img class="img-fluid rounded z-depth-0" src="/assets/img/jax/output_17_0.png" />

  </picture><figcaption class="caption">selu</figcaption>

</figure>

    </div>
    <div class="col-sm mt-3 mt-md-0">
	<figure>

  <picture>
    <source media="(max-width: 480px)" srcset="/assets/img/jax/output_17_1-480.webp" />
    <source media="(max-width: 800px)" srcset="/assets/img/jax/output_17_1-800.webp" />
    <source media="(max-width: 1400px)" srcset="/assets/img/jax/output_17_1-1400.webp" />
    <!-- Fallback to the original file -->
    <img class="img-fluid rounded z-depth-0" src="/assets/img/jax/output_17_1.png" />

  </picture><figcaption class="caption">sigmoid</figcaption>

</figure>

    </div></div>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
</pre></td><td class="code"><pre>  <span class="n">data</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">normal</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="p">(</span><span class="mi">1000000</span><span class="p">,))</span>

  <span class="k">print</span><span class="p">(</span><span class="s">'non-jit version:'</span><span class="p">)</span>
  <span class="o">%</span><span class="n">timeit</span> <span class="n">selu</span><span class="p">(</span><span class="n">data</span><span class="p">).</span><span class="n">block_until_ready</span><span class="p">()</span>
</pre></td></tr></tbody></table></code></pre></figure>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>non-jit version:
1.42 ms ± 163 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)
</code></pre></div></div>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
</pre></td><td class="code"><pre>  <span class="k">print</span><span class="p">(</span><span class="s">'jit version:'</span><span class="p">)</span>
  <span class="o">%</span><span class="n">timeit</span> <span class="n">selu_jit</span><span class="p">(</span><span class="n">data</span><span class="p">).</span><span class="n">block_until_ready</span><span class="p">()</span>
</pre></td></tr></tbody></table></code></pre></figure>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>jit version:
82.2 µs ± 24.4 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)
</code></pre></div></div>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
</pre></td><td class="code"><pre>  <span class="k">print</span><span class="p">(</span><span class="s">'non-jit version:'</span><span class="p">)</span>
  <span class="o">%</span><span class="n">timeit</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">data</span><span class="p">).</span><span class="n">block_until_ready</span><span class="p">()</span>
  <span class="k">print</span><span class="p">(</span><span class="s">"jit version"</span><span class="p">)</span>
  <span class="o">%</span><span class="n">timeit</span> <span class="n">sigmoid_jit</span><span class="p">(</span><span class="n">data</span><span class="p">).</span><span class="n">block_until_ready</span><span class="p">()</span>
</pre></td></tr></tbody></table></code></pre></figure>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>non-jit version:
488 µs ± 33.2 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)
jit version
43.3 µs ± 206 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)
</code></pre></div></div>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
</pre></td><td class="code"><pre>  <span class="k">print</span><span class="p">(</span><span class="s">'non-jit version'</span><span class="p">)</span>
  <span class="o">%</span><span class="n">timeit</span> <span class="n">add</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">data</span><span class="p">).</span><span class="n">block_until_ready</span><span class="p">()</span>
</pre></td></tr></tbody></table></code></pre></figure>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>non-jit version
292 µs ± 10.1 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
</code></pre></div></div>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
</pre></td><td class="code"><pre>  <span class="k">print</span><span class="p">(</span><span class="s">'jit version'</span><span class="p">)</span>
  <span class="o">%</span><span class="n">timeit</span> <span class="n">add_jit</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">data</span><span class="p">).</span><span class="n">block_until_ready</span><span class="p">()</span>
</pre></td></tr></tbody></table></code></pre></figure>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>jit version
104 µs ± 24.6 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)
</code></pre></div></div>

<p>The speedup of <code class="language-plaintext highlighter-rouge">jit</code> is achieved using the cached version of a function which is jit-compiled when it is called for first time. <code class="language-plaintext highlighter-rouge">jax</code> is also equipped with <code class="language-plaintext highlighter-rouge">vmap</code> and <code class="language-plaintext highlighter-rouge">pmap</code> directives for vectoriazation which makes execution even faster. We will look deeply into these functions in the next tutorial. Have fun.!! :robot:</p>]]></content><author><name></name></author><category term="tutorial" /><summary type="html"><![CDATA[A very quick intro into jax, a "GPU powered numpy"]]></summary></entry></feed>