---
layout: post
title: JAX. A quick peek
categories: tutorial
description: A very quick intro into jax, a "GPU powered numpy"
---

# Introduction to JAX

JAX is a GPU/TPU accelerated API developed by Google, that allows transformations and manipulation of Numpy-like arrays. Essentially, JAX is a Numpy on steroids.

Jax make use of XLA (Accelerated Linear Algebra) to compile and run code on hardware accelerators like GPU/TPU. Where Autograd allows for automatic differentiation (both forward and reverse) of python function to arbitrary order. JAX system make use of just-in-time compiler to generate code for pure-and-statically-composed subroutines. It can perform composible transformations of numpy array to differentiate, vectorize and parallalize, just-in-time compilation and much much more. 
Let us begin.

## Installation
JAX for CPU can be installed by running the following command on a shell. JAX depends on XLA, which needs to be installed as `jaxlib` package. Use the following instruction.
```bash
   $ pip install --upgrade pip
   $ pip install --upgrade "jax[cpu]"
```
JAX with NVidia GPU support, require CUDA and CUDNN. JAX is not bundled with CUDA and CUDNN with pip package. JAX provides pre-built CUDA-compatible wheels for Linux only, with CUDA 11.1 or newer, and CuDNN 8.0.5 or newer. Other combinations of operating system, CUDA, and CuDNN are possible, but require building from source. A detailed instruction can be found [here](https://github.com/google/jax#installation)
```bash
  $ pip install --upgrade pip
  $ pip install --upgrade "jax[cuda]" -f https://storage.googleapis.com/jax-releases/jax_releases.html  
```
### Check if `jax` is using GPU
In order to check whether `jaxlib` identifies your hardware accelerator. It can be done in any of the following method

{% highlight python linenos %}
  import jax
  jax.default_backend()
{% endhighlight %}

```
gpu
```
{% highlight python linenos %}
  jax.devices()
{% endhighlight %}

    [GpuDevice(id=0, process_index=0), GpuDevice(id=1, process_index=0)]

{% highlight python linenos %}
  jax.device_count() # shows number of devices
{% endhighlight %}

    2

## Jax-numpy vs Numpy
JAX-numpy and regular numpy are very very similar in terms of API. Most operations in numpy is that is available in numpy is implemented in JAX-numpy also. We can just take a look at a few.

{% highlight python linenos %}

  import numpy as np
  import jax.numpy as jnp

  array_np = np.arange(10, dtype=np.int32)
  array_jnp = jnp.arange(10, dtype=jnp.int32)

  print("Numpy array: ", array_np)
  print("JAX array: ", array_jnp)

  print("numpy array type:", type(array_np))
  print("jax-numpy array type: ", type(array_jnp))

{% endhighlight %}

```
Numpy array:  [0 1 2 3 4 5 6 7 8 9]
JAX array:  [0 1 2 3 4 5 6 7 8 9]
type numpy:  <class 'numpy.ndarray'>
type jax-numpy:  <class 'jaxlib.xla_extension.DeviceArray'>

```
Lets plot some graphs.

{% highlight python linenos %}

  import jax
  import jax.numpy as jnp
  from jax import jit 
  import numpy as np
  from matplotlib import pyplot as plt

{% endhighlight %}

{% highlight python linenos %}

  x_np = np.linspace(1, 10, 1000)
  y_np = np.sin(x_np) + x_np ** 0.5
  plt.plot(x_np, y_np)
  plt.show()

{% endhighlight %}

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
	{% include figure.html path="assets/img/jax/output_1_0.png" class="img-fluid rounded z-depth-0" %}
    </div>
</div>

 
    
{% highlight python linenos %}

  x_jnp = jnp.linspace(1, 10, 1000)
  y_jnp = jnp.sin(x_jnp) + x_jnp ** 0.5
  plt.plot(x_jnp, y_jnp)
  plt.show()

{% endhighlight %}

 
<div class="row">
  {% include figure.html path="assets/img/jax/output_1_0.png" class="rounded z-depth-0" %}
</div>

    
    
This is just a sample on the parallel between JAX and numpy. The real power of JAX comes with its ability to use hardware accelerators for operations. Lets look at the following example.



{% highlight python linenos %}

  seed = 0
  key = jax.random.PRNGKey(seed)
  size = 5000

  x_jnp = jax.random.normal(key, (size, size), dtype=jnp.float32)
  x_np = np.random.normal(size=(size, size)).astype(np.float32)


  %timeit jnp.dot(x_jnp, x_jnp.T).block_until_ready()
  %timeit np.dot(x_np, x_np.T)
  %timeit jnp.dot(x_np, x_np.T).block_until_ready()

{% endhighlight %}

    27.8 ms ± 131 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)
    355 ms ± 780 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)
    120 ms ± 14.6 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)

We can see the drastic performance change in the first and second line of code. Where the first one uses JAX device array and JAX implimentation of dot product which uses GPU for computation and second one uses regular numpy array with no hardware acceleration. The third line of code uses jax operations on numpy arrays. We will visit a detailed analysis on this in the future. For now lets focus on the performance gain using `jax`

### Accelerate using `jit()` function
`jit()` is special function for accelerating jax executions using just-in-time compilation and vectorization respectively. We will cover deeply on these topics on upcoming posts. For now lets see what kind of performance improvements we are expecting. 

#### Just-in-time, `jit()` compilation
Let us start by defining some helper functions and some jax-numpy vectors

{% highlight python linenos %}

  size = 1000

  x_jnp = jax.random.normal(key, (size, size), dtype=jnp.float32)


  def selu(x, alpha=1.67, lmbda=1.05):  # note: SELU is an activation function
    return lmbda * jnp.where(x > 0, x, alpha * jnp.exp(x) - alpha)

  def sigmoid(x):
    return 1/(1 + jnp.exp(-x))


  def add(x, y):
    return jnp.add(x, y) * jnp.exp(x) - 2.05 + y * 0.56

  # Simple helper visualization function

  def visualize_fn(fn, l=-10, r=10, n=1000):
    x = np.linspace(l, r, num=n)
    y = fn(x)
    plt.plot(x, y); plt.show()


  sigmoid_jit = jit(sigmoid)
  selu_jit = jit(selu)
  add_jit = jit(add)
  visualize_fn(selu)
  visualize_fn(sigmoid)

{% endhighlight %}


Once the functions are defined we use `@jit` decorator for speedup. Normally JAX dispatches tasks to GPU one operation at a time. But if we have a sequence of operation, we can compile multiple operations together using XLA and despatch to GPU.
    

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
	{% include figure.html path="assets/img/jax/output_17_0.png" class="img-fluid rounded z-depth-0" caption="selu" %}
    </div>
    <div class="col-sm mt-3 mt-md-0">
	{% include figure.html path="assets/img/jax/output_17_1.png" class="img-fluid rounded z-depth-0" caption="sigmoid" %}
    </div></div>



{% highlight python linenos %}

  data = jax.random.normal(key, (1000000,))

  print('non-jit version:')
  %timeit selu(data).block_until_ready()

{% endhighlight %}

    non-jit version:
    1.42 ms ± 163 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)



{% highlight python linenos %}

  print('jit version:')
  %timeit selu_jit(data).block_until_ready()

{% endhighlight %}


    jit version:
    82.2 µs ± 24.4 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)



{% highlight python linenos %}
  print('non-jit version:')
  %timeit sigmoid(data).block_until_ready()
  print("jit version")
  %timeit sigmoid_jit(data).block_until_ready()
{% endhighlight %}


    non-jit version:
    488 µs ± 33.2 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)
    jit version
    43.3 µs ± 206 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)



{% highlight python linenos %}

  print('non-jit version')
  %timeit add(data, data).block_until_ready()
{% endhighlight %}

    non-jit version
    292 µs ± 10.1 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)



{% highlight python linenos %}
  print('jit version')
  %timeit add_jit(data, data).block_until_ready()
{% endhighlight %}

    jit version
    104 µs ± 24.6 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)


The speedup of `jit` is achieved using the cached version of a function which is jit-compiled when it is called for first time. `jax` is also equipped with `vmap` and `pmap` directives for vectoriazation which makes execution even faster. We will look deeply into these functions in the next tutorial. Have fun.!! :robot:




